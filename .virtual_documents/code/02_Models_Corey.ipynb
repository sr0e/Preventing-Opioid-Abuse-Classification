


#pip install imbalanced-learn


#!pip install xgboost


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import xgboost as xgb

from sklearn.dummy import DummyClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.datasets import make_classification
from collections import Counter
from imblearn.over_sampling import RandomOverSampler


pain = pd.read_csv(r'C:\Users\rockm\Desktop\GA\Projects\project-5\data\pain_clean.csv')


pain.shape


X = pain.drop('PRLMISEVR', axis=1)
y = pain['PRLMISEVR']


ros = RandomOverSampler(random_state=42)


X_resampled, y_resampled = ros.fit_resample(X, y)


print(f'Resampled dataset shape: {Counter(y_resampled)}')


# There's a large class imbalance of about 90/10 for my target variable, so I'm going to oversample the minority group


resampled_data = pd.DataFrame(X_resampled, columns=X.columns)
resampled_data['PRLMISEVR'] = y_resampled


resampled_data.head()


resampled_data['PRLMISEVR'].value_counts()


resampled_data.describe()


resampled_data.shape





X = resampled_data.drop(columns=['MARRIED', 'EDUCAT', 'CTYMETRO', 'PRLMISAB', 'PRLANY', 'PRLMISEVR'])
y = resampled_data['PRLMISEVR']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


rf = RandomForestClassifier(n_estimators=100, random_state=42)


rf.fit(X_train, y_train)


y_pred = rf.predict(X_test)


print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))


rf.score(X_test, y_test)





X = resampled_data.drop(columns=['MARRIED', 'EDUCAT', 'CTYMETRO', 'PRLMISAB', 'PRLANY', 'PRLMISEVR'])
y = resampled_data['PRLMISEVR']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}


rf2 = RandomForestClassifier(random_state=42)


grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)


grid_search.fit(X_train, y_train)


print(f'Best parameters found: {grid_search.best_params_}')
print(f'Best cross-validation score: {grid_search.best_score_}')


rf2 = RandomForestClassifier(
    bootstrap=False,
    max_depth=30,
    min_samples_leaf=1,
    min_samples_split=2,
    n_estimators=200,
    random_state=42
)


rf2.fit(X_train, y_train)


rf2.score(X_test, y_test)





logr = LogisticRegression(random_state=42)


logr.fit(X_train, y_train)


logr.score(X_test, y_test)





from sklearn.svm import SVC


svm = SVC(kernel='linear', random_state=42)


# For some reason I couldn't get this to ever finish running no matter what I tried.
svm.fit(X_train, y_train)


y_pred = svm.predict(X_test)


print("SVM Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))





X = resampled_data.drop(columns=['MARRIED', 'EDUCAT', 'CTYMETRO', 'PRLMISAB', 'PRLANY', 'PRLMISEVR'])
y = resampled_data['PRLMISEVR']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


seq = Sequential()
seq.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
seq.add(Dropout(0.5))
seq.add(Dense(64, activation='relu'))
seq.add(Dropout(0.5))
seq.add(Dense(32, activation='relu'))
seq.add(Dense(1, activation='sigmoid'))


seq.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)



seq.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), 
          callbacks=[early_stopping], verbose=1)





X = resampled_data.drop(columns=['PRLMISAB', 'PRLANY', 'PRLMISEVR'])
y = resampled_data['PRLMISEVR']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


pca = PCA(n_components=15) 
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)


print("Explained variance ratio:", pca.explained_variance_ratio_)


rf_model = RandomForestClassifier(
    bootstrap=False,
    max_depth=30,
    min_samples_leaf=1,
    min_samples_split=2,
    n_estimators=200,
    random_state=42
)


rf_model.fit(X_train_pca, y_train)


y_pred = rf_model.predict(X_test_pca)


print("Random Forest Accuracy with PCA:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))


from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


cm = confusion_matrix(y_test, y_pred)
cm


disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()


rf_model.score(X_test_pca, y_test)





X = resampled_data.drop(columns=['PRLMISAB', 'PRLANY', 'PRLMISEVR'])
y = resampled_data['PRLMISEVR']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


pca = PCA(n_components=10) 
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)


from sklearn.ensemble import VotingClassifier


gb_model = GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=42)
xgb_model = xgb.XGBClassifier(n_estimators=200, max_depth=5, random_state=42)


voting_model = VotingClassifier(estimators=[('rf', rf_model), ('gb', gb_model), ('xgb', xgb_model)], voting='soft')


voting_model.fit(X_train_pca, y_train)


y_pred = voting_model.predict(X_test_pca)


print("Voting Classifier Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))


voting_model.score(X_test_pca, y_test)





X = resampled_data[['MENTHLTH', 'UPPERS', 'DOWNERS', 'HISTORY', 'REPORTED_HEALTH', 'TRTMENT', 'HALUCNG', 'AMPHETMN', 'COCAINE', 'TRQLZRS', 'HEROINUSE', 'HEROINEVR', 'PRLANY']]
y = resampled_data['PRLMISEVR']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


pca = PCA(n_components=10) 
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)


gb_model = GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=42)
xgb_model = xgb.XGBClassifier(n_estimators=200, max_depth=5, random_state=42)


voting_model2 = VotingClassifier(estimators=[('rf', rf_model), ('gb', gb_model), ('xgb', xgb_model)], voting='soft')


voting_model2.fit(X_train_pca, y_train)


y_pred = voting_model.predict(X_test_pca)


print("Voting Classifier Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))


voting_model2.score(X_test_pca, y_test)





X = resampled_data.drop(columns=['PRLMISAB', 'PRLANY', 'PRLMISEVR'])
y = resampled_data['PRLMISAB']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


pca = PCA(n_components=15) 
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)


rf3 = RandomForestClassifier(
    bootstrap=False,
    max_depth=30,
    min_samples_leaf=1,
    min_samples_split=2,
    n_estimators=200,
    random_state=42
)


rf3.fit(X_train_pca, y_train)


y_pred = rf3.predict(X_test_pca)


print("Voting Classifier Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))


rf3.score(X_test_pca, y_test)



